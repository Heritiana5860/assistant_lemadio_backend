# """
# Script pour cr√©er les embeddings de la documentation ADES
# et les stocker dans FAISS pour la recherche s√©mantique
# """

# from sentence_transformers import SentenceTransformer
# import faiss
# import pickle
# import numpy as np
# import os

# # ============== CONFIGURATION ==============

# # Chemin vers la documentation
# # DOC_PATH = 'docs/ADES_DOCUMENTATION.md'
# DOC_PATH = 'docs/Creation_vente.md'

# # Dossier de sortie pour les fichiers g√©n√©r√©s
# OUTPUT_DIR = 'rag/data'

# # Mod√®le d'embedding (multilangue, bon pour fran√ßais/malagasy)
# EMBEDDING_MODEL = 'all-MiniLM-L6-v2'

# # Taille des chunks (en caract√®res approximatif)
# CHUNK_SIZE = 500

# # ============== FONCTIONS ==============

# def load_documentation(file_path):
#     """
#     Charge la documentation depuis le fichier Markdown
#     """
#     print(f"üìñ Chargement de la documentation depuis {file_path}...")
    
#     try:
#         with open(file_path, 'r', encoding='utf-8') as f:
#             content = f.read()
#         print(f"‚úÖ Documentation charg√©e ({len(content)} caract√®res)")
#         return content
#     except FileNotFoundError:
#         print(f"‚ùå Erreur : Fichier {file_path} introuvable")
#         return None
#     except Exception as e:
#         print(f"‚ùå Erreur lors du chargement : {e}")
#         return None

# def split_into_chunks(text, chunk_size=500):
#     """
#     D√©coupe le texte en chunks intelligents
    
#     Strat√©gie :
#     1. D'abord par sections (## ou ###)
#     2. Puis par paragraphes si trop grand
#     3. Garde un peu de contexte entre chunks
#     """
#     print(f"‚úÇÔ∏è  D√©coupage en chunks de ~{chunk_size} caract√®res...")
    
#     chunks = []
    
#     # S√©parer par les titres de sections
#     sections = text.split('\n## ')
    
#     for i, section in enumerate(sections):
#         if i > 0:  # Rajouter ## sauf pour le premier
#             section = '## ' + section
        
#         # Si la section est trop grande, la d√©couper par paragraphes
#         if len(section) > chunk_size * 2:
#             paragraphs = section.split('\n\n')
#             current_chunk = ""
            
#             for para in paragraphs:
#                 # Si ajouter ce paragraphe d√©passe la limite
#                 if len(current_chunk) + len(para) > chunk_size and current_chunk:
#                     chunks.append(current_chunk.strip())
#                     # Garder un peu de contexte (overlap)
#                     current_chunk = para
#                 else:
#                     current_chunk += "\n\n" + para if current_chunk else para
            
#             # Ajouter le dernier chunk
#             if current_chunk:
#                 chunks.append(current_chunk.strip())
#         else:
#             # La section est assez petite, on la garde enti√®re
#             if section.strip():
#                 chunks.append(section.strip())
    
#     # Filtrer les chunks trop petits (moins de 50 caract√®res)
#     chunks = [c for c in chunks if len(c) > 50]
    
#     print(f"‚úÖ {len(chunks)} chunks cr√©√©s")
    
#     # Afficher quelques exemples
#     print("\nüìù Exemples de chunks cr√©√©s :")
#     for i, chunk in enumerate(chunks[:3]):
#         preview = chunk[:100].replace('\n', ' ')
#         print(f"  Chunk {i+1}: {preview}...")
    
#     return chunks

# def create_embeddings(chunks, model_name):
#     """
#     Cr√©e les embeddings pour chaque chunk
#     """
#     print(f"\nü§ñ Chargement du mod√®le d'embedding : {model_name}...")
    
#     try:
#         model = SentenceTransformer(model_name)
#         print(f"‚úÖ Mod√®le charg√©")
#     except Exception as e:
#         print(f"‚ùå Erreur lors du chargement du mod√®le : {e}")
#         return None, None
    
#     print(f"üî¢ Cr√©ation des embeddings pour {len(chunks)} chunks...")
#     print("‚è≥ Cela peut prendre 1-2 minutes...")
    
#     try:
#         # Encoder tous les chunks en une fois
#         embeddings = model.encode(chunks, show_progress_bar=True)
        
#         print(f"‚úÖ Embeddings cr√©√©s : {embeddings.shape}")
#         print(f"   - Nombre de chunks : {embeddings.shape[0]}")
#         print(f"   - Dimension des vecteurs : {embeddings.shape[1]}")
        
#         return embeddings, model
#     except Exception as e:
#         print(f"‚ùå Erreur lors de la cr√©ation des embeddings : {e}")
#         return None, None

# def create_faiss_index(embeddings):
#     """
#     Cr√©e un index FAISS pour la recherche rapide
#     """
#     print(f"\nüóÑÔ∏è  Cr√©ation de l'index FAISS...")
    
#     try:
#         # Dimension des vecteurs
#         dimension = embeddings.shape[1]
        
#         # Cr√©er un index simple (L2 distance)
#         index = faiss.IndexFlatL2(dimension)
        
#         # Ajouter les embeddings √† l'index
#         index.add(embeddings.astype('float32'))
        
#         print(f"‚úÖ Index FAISS cr√©√© avec {index.ntotal} vecteurs")
        
#         return index
#     except Exception as e:
#         print(f"‚ùå Erreur lors de la cr√©ation de l'index : {e}")
#         return None

# def save_artifacts(index, chunks, output_dir):
#     """
#     Sauvegarde l'index FAISS et les chunks
#     """
#     print(f"\nüíæ Sauvegarde des fichiers dans {output_dir}...")
    
#     # Cr√©er le dossier s'il n'existe pas
#     os.makedirs(output_dir, exist_ok=True)
    
#     try:
#         # Sauvegarder l'index FAISS
#         index_path = os.path.join(output_dir, 'faiss_index.index')
#         faiss.write_index(index, index_path)
#         print(f"‚úÖ Index FAISS sauvegard√© : {index_path}")
        
#         # Sauvegarder les chunks
#         chunks_path = os.path.join(output_dir, 'chunks.pkl')
#         with open(chunks_path, 'wb') as f:
#             pickle.dump(chunks, f)
#         print(f"‚úÖ Chunks sauvegard√©s : {chunks_path}")
        
#         # Sauvegarder les m√©tadonn√©es
#         metadata = {
#             'num_chunks': len(chunks),
#             'embedding_model': EMBEDDING_MODEL,
#             'chunk_size': CHUNK_SIZE,
#             'dimension': index.d
#         }
        
#         metadata_path = os.path.join(output_dir, 'metadata.pkl')
#         with open(metadata_path, 'wb') as f:
#             pickle.dump(metadata, f)
#         print(f"‚úÖ M√©tadonn√©es sauvegard√©es : {metadata_path}")
        
#         return True
#     except Exception as e:
#         print(f"‚ùå Erreur lors de la sauvegarde : {e}")
#         return False

# def test_search(index, chunks, model, test_query="Comment cr√©er une vente?"):
#     """
#     Teste la recherche avec une question exemple
#     """
#     print(f"\nüîç Test de recherche avec : '{test_query}'")
    
#     try:
#         # Encoder la question
#         query_embedding = model.encode([test_query])
        
#         # Rechercher les 3 chunks les plus similaires
#         distances, indices = index.search(query_embedding.astype('float32'), 3)
        
#         print(f"\nüìä R√©sultats de la recherche :")
#         for i, (dist, idx) in enumerate(zip(distances[0], indices[0])):
#             print(f"\n  R√©sultat {i+1} (distance: {dist:.4f}):")
#             chunk_preview = chunks[idx][:200].replace('\n', ' ')
#             print(f"  {chunk_preview}...")
        
#         return True
#     except Exception as e:
#         print(f"‚ùå Erreur lors du test : {e}")
#         return False

# # ============== MAIN ==============

# def main():
#     """
#     Fonction principale
#     """
#     print("=" * 60)
#     print("üöÄ CR√âATION DES EMBEDDINGS POUR ADES CHATBOT")
#     print("=" * 60)
    
#     # 1. Charger la documentation
#     documentation = load_documentation(DOC_PATH)
#     if documentation is None:
#         return
    
#     # 2. D√©couper en chunks
#     chunks = split_into_chunks(documentation, CHUNK_SIZE)
#     if not chunks:
#         print("‚ùå Aucun chunk cr√©√©")
#         return
    
#     # 3. Cr√©er les embeddings
#     embeddings, model = create_embeddings(chunks, EMBEDDING_MODEL)
#     if embeddings is None:
#         return
    
#     # 4. Cr√©er l'index FAISS
#     index = create_faiss_index(embeddings)
#     if index is None:
#         return
    
#     # 5. Sauvegarder
#     success = save_artifacts(index, chunks, OUTPUT_DIR)
#     if not success:
#         return
    
#     # 6. Tester
#     test_search(index, chunks, model)
    
#     print("\n" + "=" * 60)
#     print("‚úÖ TERMIN√â AVEC SUCC√àS !")
#     print("=" * 60)
#     print(f"\nüìÅ Fichiers cr√©√©s dans {OUTPUT_DIR}/")
#     print("  - faiss_index.index (index de recherche)")
#     print("  - chunks.pkl (morceaux de documentation)")
#     print("  - metadata.pkl (informations)")
#     print("\nüéâ Vous pouvez maintenant passer √† l'int√©gration dans app.py")

# if __name__ == "__main__":
#     main()

"""
Script pour cr√©er les embeddings de la documentation ADES
et les stocker dans FAISS pour la recherche s√©mantique.

Am√©liorations RAG :
1. Traite un dossier complet de fichiers Markdown.
2. Ajoute la source (nom du fichier) √† chaque chunk.
3. Impl√©mente la r√©tention d'en-t√™te : chaque chunk garde en m√©moire
   ses titres H1 et H2 parents pour un contexte accru (Header Retention).
"""

from sentence_transformers import SentenceTransformer
import faiss
import pickle
import numpy as np
import os
import re
from glob import glob

# ============== CONFIGURATION ==============

# Dossier contenant tous les fichiers Markdown
DOCS_DIR = 'docs' 

# Dossier de sortie pour les fichiers g√©n√©r√©s
OUTPUT_DIR = 'rag/data'

# Mod√®le d'embedding (multilangue, bon pour fran√ßais/malagasy)
# 'all-MiniLM-L6-v2' est excellent pour la vitesse et la qualit√©
EMBEDDING_MODEL = 'all-MiniLM-L6-v2' 

# Taille maximale souhait√©e des chunks (en caract√®res)
CHUNK_SIZE = 800
CHUNK_OVERLAP = 150

# ============== FONCTIONS ==============

def process_markdown_file(file_path, chunk_size=800):
    """
    Charge, d√©coupe et enrichit le contenu d'un seul fichier Markdown.
    Chaque chunk est un dictionnaire avec le texte et sa source.
    """
    file_name = os.path.basename(file_path)
    print(f"  üìñ Traitement de {file_name}...")
    
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
    except Exception as e:
        print(f"  ‚ùå Erreur lors du chargement de {file_name}: {e}")
        return []

    processed_chunks = []
    
    # S√©parer par H2 (##) pour commencer les sections logiques
    sections = content.split('\n## ')
    
    # Le premier √©l√©ment est souvent l'introduction (H1)
    current_h1 = ""
    if sections[0].strip().startswith('# '):
        match = re.search(r'#\s*(.+)', sections[0])
        if match:
            current_h1 = match.group(1).strip()
        
    for i, section_content in enumerate(sections):
        if i > 0: 
            # Si ce n'est pas le premier bloc, on rajoute le ## pour le titre
            section_content = '## ' + section_content

        # Extrait l'H2 de la section ou r√©utilise le dernier H1
        current_h2 = ""
        h2_match = re.search(r'##\s*(.+)', section_content)
        if h2_match:
            current_h2 = h2_match.group(1).strip()
        
        
        # On d√©coupe ensuite cette section par paragraphes (double saut de ligne)
        paragraphs = section_content.split('\n\n')
        current_chunk_text = ""
        
        # Le pr√©fixe de contexte (pour Header Retention)
        context_prefix = f"Source: {file_name} | {current_h1}"
        if current_h2:
            context_prefix += f" | {current_h2}: "
        else:
            context_prefix += ": " # Si c'est l'intro ou un bloc sans H2/H3

        for para in paragraphs:
            para = para.strip()
            if not para:
                continue

            # Si ajouter ce paragraphe d√©passe la limite, on finalise le chunk pr√©c√©dent
            if len(current_chunk_text) + len(para) > chunk_size and current_chunk_text:
                # Ajout du chunk avec son contexte
                processed_chunks.append({
                    'text': context_prefix + current_chunk_text.strip(),
                    'source': file_name
                })
                # Le nouveau chunk commence avec ce paragraphe
                current_chunk_text = para
            else:
                # Sinon, on continue d'ajouter au chunk courant
                current_chunk_text += "\n\n" + para if current_chunk_text else para
        
        # Ajouter le dernier chunk de la section
        if current_chunk_text:
            processed_chunks.append({
                'text': context_prefix + current_chunk_text.strip(),
                'source': file_name
            })
            
    return processed_chunks

def load_and_split_all_docs(docs_dir, chunk_size=800):
    """
    Charge et d√©coupe tous les documents Markdown du dossier sp√©cifi√©.
    """
    all_chunks = []
    
    print(f"‚úÇÔ∏è  D√©coupage des documents dans le dossier '{docs_dir}'...")
    
    # Utiliser glob pour trouver tous les fichiers .md
    markdown_files = glob(os.path.join(docs_dir, '*.md'))
    
    if not markdown_files:
        print(f"‚ùå Aucun fichier Markdown (.md) trouv√© dans {docs_dir}. V√©rifiez le chemin.")
        return []

    for file_path in markdown_files:
        chunks_from_file = process_markdown_file(file_path, chunk_size)
        all_chunks.extend(chunks_from_file)

    # Filtrer les chunks trop petits (moins de 50 caract√®res)
    all_chunks = [c for c in all_chunks if len(c['text']) > 50]

    print(f"‚úÖ {len(all_chunks)} chunks totaux cr√©√©s √† partir de {len(markdown_files)} fichiers.")
    
    # Afficher quelques exemples
    print("\nüìù Exemples de chunks cr√©√©s (avec r√©tention d'en-t√™te) :")
    for i, chunk in enumerate(all_chunks[:3]):
        preview = chunk['text'][:150].replace('\n', ' ')
        print(f"  Chunk {i+1} (Source: {chunk['source']}): {preview}...")
    
    return all_chunks

def create_embeddings(chunks, model_name):
    """
    Cr√©e les embeddings pour chaque chunk
    """
    print(f"\nü§ñ Chargement du mod√®le d'embedding : {model_name}...")
    
    try:
        model = SentenceTransformer(model_name)
        print(f"‚úÖ Mod√®le charg√©")
    except Exception as e:
        print(f"‚ùå Erreur lors du chargement du mod√®le : {e}")
        return None, None
    
    texts_to_embed = [c['text'] for c in chunks]
    
    print(f"üî¢ Cr√©ation des embeddings pour {len(texts_to_embed)} chunks...")
    print("‚è≥ Cela peut prendre quelques instants...")
    
    try:
        # Encoder tous les chunks en une fois
        embeddings = model.encode(texts_to_embed, show_progress_bar=True)
        
        print(f"‚úÖ Embeddings cr√©√©s : {embeddings.shape}")
        return embeddings, model
    except Exception as e:
        print(f"‚ùå Erreur lors de la cr√©ation des embeddings : {e}")
        return None, None

def create_faiss_index(embeddings):
    """
    Cr√©e un index FAISS pour la recherche rapide
    """
    print(f"\nüóÑÔ∏è  Cr√©ation de l'index FAISS...")
    
    try:
        # Dimension des vecteurs
        dimension = embeddings.shape[1]
        
        # Cr√©er un index simple (L2 distance)
        index = faiss.IndexFlatL2(dimension)
        
        # Ajouter les embeddings √† l'index
        index.add(embeddings.astype('float32'))
        
        print(f"‚úÖ Index FAISS cr√©√© avec {index.ntotal} vecteurs")
        return index
    except Exception as e:
        print(f"‚ùå Erreur lors de la cr√©ation de l'index : {e}")
        return None

def save_artifacts(index, chunks, output_dir):
    """
    Sauvegarde l'index FAISS et les chunks
    """
    print(f"\nüíæ Sauvegarde des fichiers dans {output_dir}...")
    
    # Cr√©er le dossier s'il n'existe pas
    os.makedirs(output_dir, exist_ok=True)
    
    try:
        # Sauvegarder l'index FAISS
        index_path = os.path.join(output_dir, 'faiss_index.index')
        faiss.write_index(index, index_path)
        print(f"‚úÖ Index FAISS sauvegard√© : {index_path}")
        
        # Sauvegarder les chunks (qui incluent la source et le texte)
        chunks_path = os.path.join(output_dir, 'chunks.pkl')
        with open(chunks_path, 'wb') as f:
            pickle.dump(chunks, f)
        print(f"‚úÖ Chunks et M√©tadonn√©es sauvegard√©s : {chunks_path}")
        
        return True
    except Exception as e:
        print(f"‚ùå Erreur lors de la sauvegarde : {e}")
        return False

def test_search(index, chunks, model, test_query="Qui me donne mon mot de passe et mon identifiant ?"):
    """
    Teste la recherche avec une question exemple
    """
    print(f"\nüîç Test de recherche avec : '{test_query}'")
    
    try:
        # Encoder la question
        query_embedding = model.encode([test_query])
        
        # Rechercher les 3 chunks les plus similaires
        distances, indices = index.search(query_embedding.astype('float32'), 3)
        
        print(f"\nüìä R√©sultats de la recherche (3 meilleurs chunks) :")
        for i, (dist, idx) in enumerate(zip(distances[0], indices[0])):
            print(f"\n  R√©sultat {i+1} (distance: {dist:.4f}):")
            chunk_preview = chunks[idx]['text'].replace('\n', ' ')
            print(f"  Source: {chunks[idx]['source']}")
            print(f"  Contenu : {chunk_preview[:200]}...")
        
        return True
    except Exception as e:
        print(f"‚ùå Erreur lors du test : {e}")
        return False
    
def smart_chunk_with_overlap(text, chunk_size=800, overlap=150):
    """D√©coupe avec chevauchement et respect des phrases"""
    sentences = re.split(r'(?<=[.!?])\s+', text)
    chunks = []
    current_chunk = ""
    
    for sentence in sentences:
        if len(current_chunk) + len(sentence) <= chunk_size:
            current_chunk += sentence + " "
        else:
            if current_chunk:
                chunks.append(current_chunk.strip())
                # Overlap : garder les derniers X caract√®res
                current_chunk = current_chunk[-overlap:] + sentence + " "
    
    if current_chunk:
        chunks.append(current_chunk.strip())
    
    return chunks

def extract_hierarchy(section_content):
    """Extrait TOUS les niveaux de titres (H1->H6)"""
    headers = {
        'h1': re.search(r'^#\s+(.+)$', section_content, re.MULTILINE),
        'h2': re.search(r'^##\s+(.+)$', section_content, re.MULTILINE),
        'h3': re.search(r'^###\s+(.+)$', section_content, re.MULTILINE)
    }
    
    hierarchy = " > ".join([
        h.group(1) for h in [headers['h1'], headers['h2'], headers['h3']] 
        if h
    ])
    
    return hierarchy

# ============== MAIN ==============

def main():
    """
    Fonction principale
    """
    print("=" * 60)
    print("üöÄ CR√âATION DE LA BASE DE VECTEURS POUR ADES CHATBOT")
    print("=" * 60)
    
    # 1. Charger et d√©couper en chunks (avec source et en-t√™te)
    chunks_with_metadata = load_and_split_all_docs(DOCS_DIR, CHUNK_SIZE)
    if not chunks_with_metadata:
        return
    
    # 2. Cr√©er les embeddings
    embeddings, model = create_embeddings(chunks_with_metadata, EMBEDDING_MODEL)
    if embeddings is None:
        return
    
    # 3. Cr√©er l'index FAISS
    index = create_faiss_index(embeddings)
    if index is None:
        return
    
    # 4. Sauvegarder
    success = save_artifacts(index, chunks_with_metadata, OUTPUT_DIR)
    if not success:
        return
    
    # 5. Tester
    # N√©cessite un mod√®le charg√©, donc appel dans le main
    test_search(index, chunks_with_metadata, model)
    
    print("\n" + "=" * 60)
    print("‚úÖ TERMIN√â AVEC SUCC√àS ! Votre base de vecteurs est optimis√©e.")
    print("=" * 60)

if __name__ == "__main__":
    main()