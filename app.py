# from flask import Flask, request, jsonify
# from flask_cors import CORS
# import requests
# import json
# import datetime
# import os
# from dotenv import load_dotenv
# from rag_search import RAGSearcher

# # Charger les variables d'environnement
# load_dotenv()

# # Cr√©er l'application Flask
# app = Flask(__name__)

# # Activer CORS (permet √† Flutter de communiquer)
# CORS(app)

# # ============== CONFIGURATION ==============

# # URL de Ollama (locale par d√©faut)
# OLLAMA_URL = os.getenv('OLLAMA_URL', 'http://localhost:11434')
# OLLAMA_MODEL = 'mistral'

# # System prompt : c'est les "instructions" qu'on donne √† Mistral
# SYSTEM_PROMPT = """Tu es un assistant d'aide pour l'application mobile ADES.
# Tu aides les vendeurs √† utiliser l'application de vente ADES.

# R√àGLES IMPORTANTES :
# - Base tes r√©ponses UNIQUEMENT sur la documentation fournie ci-dessous
# - Sois pr√©cis, clair et concis
# - R√©ponds en fran√ßais ou malagasy selon la question
# - Si tu ne trouves pas l'information dans la documentation, dis-le honn√™tement
# - Donne des instructions √©tape par √©tape quand c'est appropri√©
# - Si la question n'est pas sur l'app ADES, dis poliment que tu ne peux pas aider"""

# # Cr√©er le dossier logs s'il n'existe pas
# if not os.path.exists('logs'):
#     os.makedirs('logs')

# # ============== INITIALISATION RAG ==============

# print("\n" + "="*60)
# print("üöÄ D√âMARRAGE DU BACKEND ADES CHATBOT")
# print("="*60)

# # Initialiser le RAG Searcher
# try:
#     rag_searcher = RAGSearcher(data_dir='rag/data')
#     RAG_ENABLED = True
#     print("‚úÖ RAG activ√© et pr√™t")
# except Exception as e:
#     print(f"‚ö†Ô∏è  RAG d√©sactiv√© : {e}")
#     print("   Le chatbot fonctionnera sans contexte personnalis√©")
#     rag_searcher = None
#     RAG_ENABLED = False

# print("="*60 + "\n")

# # ============== FONCTIONS UTILITAIRES ==============

# def log_conversation(user_message, bot_response, context_used=None):
#     """
#     Sauvegarde chaque conversation dans un fichier log
#     pour pouvoir analyser plus tard
#     """
#     timestamp = datetime.datetime.now().isoformat()
#     log_entry = {
#         'timestamp': timestamp,
#         'user': user_message,
#         'bot': bot_response,
#         'context_used': context_used is not None,
#         'rag_enabled': RAG_ENABLED
#     }
    
#     log_file = 'logs/conversations.jsonl'
    
#     try:
#         with open(log_file, 'a', encoding='utf-8') as f:
#             f.write(json.dumps(log_entry, ensure_ascii=False) + '\n')
#     except Exception as e:
#         print(f"Erreur lors du logging: {e}")

# def call_ollama(prompt, temperature=0.3):
#     """
#     Appelle Ollama (Mistral) avec le prompt fourni
#     Retourne la r√©ponse
#     """
#     try:
#         # Pr√©parer la requ√™te pour Ollama
#         payload = {
#             'model': OLLAMA_MODEL,
#             'prompt': prompt,
#             'stream': False,
#             'temperature': temperature
#         }
        
#         # Appeler l'API Ollama
#         response = requests.post(
#             f'{OLLAMA_URL}/api/generate',
#             json=payload,
#             timeout=60  # Timeout apr√®s 60 secondes
#         )
        
#         # V√©rifier si l'appel a r√©ussi
#         if response.status_code == 200:
#             result = response.json()
#             return result.get('response', 'Erreur: pas de r√©ponse')
#         else:
#             return f'Erreur Ollama: {response.status_code}'
            
#     except requests.exceptions.ConnectionError:
#         return 'Erreur: Impossible de se connecter √† Ollama. V√©rifiez que Ollama fonctionne (ollama serve)'
#     except requests.exceptions.Timeout:
#         return 'Erreur: Ollama a mis trop de temps √† r√©pondre'
#     except Exception as e:
#         return f'Erreur: {str(e)}'

# def create_prompt_with_context(user_message, context):
#     """
#     Cr√©e le prompt complet avec le contexte RAG
#     """
#     prompt = f"""{SYSTEM_PROMPT}

# DOCUMENTATION ADES (extraits pertinents) :
# ---
# {context}
# ---

# QUESTION DU VENDEUR : {user_message}

# R√âPONSE (bas√©e uniquement sur la documentation ci-dessus) :"""
    
#     return prompt

# def create_prompt_without_context(user_message):
#     """
#     Cr√©e le prompt sans contexte (fallback si RAG d√©sactiv√©)
#     """
#     prompt = f"""{SYSTEM_PROMPT}

# QUESTION : {user_message}

# R√âPONSE :"""
    
#     return prompt

# # ============== ROUTES API ==============

# @app.route('/', methods=['GET'])
# def home():
#     """
#     Route simple pour v√©rifier que le serveur fonctionne
#     """
#     return jsonify({
#         'status': 'ok',
#         'message': 'Backend ADES ChatBot fonctionne!',
#         'version': '2.0.0',
#         'rag_enabled': RAG_ENABLED
#     })

# @app.route('/api/health', methods=['GET'])
# def health():
#     """
#     V√©rifie la sant√© du serveur et la connexion √† Ollama
#     """
#     try:
#         # Essayer de se connecter √† Ollama
#         response = requests.get(f'{OLLAMA_URL}/api/tags', timeout=5)
#         ollama_status = 'connected' if response.status_code == 200 else 'disconnected'
#     except:
#         ollama_status = 'disconnected'
    
#     # V√©rifier le statut du RAG
#     rag_status = 'disabled'
#     if RAG_ENABLED and rag_searcher:
#         rag_health = rag_searcher.health_check()
#         rag_status = rag_health.get('status', 'error')
    
#     return jsonify({
#         'backend': 'running',
#         'ollama': ollama_status,
#         'rag': rag_status,
#         'timestamp': datetime.datetime.now().isoformat()
#     })

# @app.route('/api/chat', methods=['POST'])
# def chat():
#     """
#     Route principale pour le chatbot avec RAG
#     Re√ßoit une question, cherche dans la doc, appelle Mistral, retourne la r√©ponse
#     """
#     try:
#         # R√©cup√©rer le message de la requ√™te
#         data = request.get_json()
#         user_message = data.get('message', '')
        
#         # Valider que le message n'est pas vide
#         if not user_message or len(user_message.strip()) == 0:
#             return jsonify({
#                 'error': 'Le message ne peut pas √™tre vide'
#             }), 400
        
#         # Variable pour stocker le contexte utilis√©
#         context_used = None
        
#         # Cr√©er le prompt avec ou sans RAG
#         if RAG_ENABLED and rag_searcher:
#             # Rechercher le contexte pertinent
#             print(f"üîç Recherche RAG pour : {user_message[:50]}...")
#             context = rag_searcher.get_context(user_message, top_k=3, max_length=2000)
#             context_used = context
            
#             # Cr√©er le prompt avec contexte
#             full_prompt = create_prompt_with_context(user_message, context)
#             print(f"‚úÖ Contexte trouv√© ({len(context)} caract√®res)")
#         else:
#             # Fallback sans RAG
#             full_prompt = create_prompt_without_context(user_message)
#             print(f"‚ö†Ô∏è  Pas de RAG, utilisation du prompt basique")
        
#         # Appeler Ollama
#         print(f"ü§ñ Appel √† Mistral...")
#         bot_response = call_ollama(full_prompt)
#         print(f"‚úÖ R√©ponse re√ßue")
        
#         # Logger la conversation
#         log_conversation(user_message, bot_response, context_used)
        
#         # Pr√©parer la r√©ponse
#         response_data = {
#             'status': 'success',
#             'user_message': user_message,
#             'reply': bot_response,
#             'rag_used': RAG_ENABLED and context_used is not None,
#             'timestamp': datetime.datetime.now().isoformat()
#         }
        
#         # Ajouter le contexte si demand√© (pour d√©boguer)
#         if data.get('include_context', False) and context_used:
#             response_data['context'] = context_used
        
#         return jsonify(response_data)
        
#     except Exception as e:
#         print(f"‚ùå Erreur dans /api/chat : {e}")
#         return jsonify({
#             'status': 'error',
#             'message': str(e)
#         }), 500

# @app.route('/api/search', methods=['POST'])
# def search():
#     """
#     Route pour tester la recherche RAG directement
#     Utile pour d√©boguer
#     """
#     try:
#         if not RAG_ENABLED or not rag_searcher:
#             return jsonify({
#                 'error': 'RAG non disponible'
#             }), 503
        
#         data = request.get_json()
#         query = data.get('query', '')
#         top_k = data.get('top_k', 3)
        
#         if not query:
#             return jsonify({
#                 'error': 'Query manquante'
#             }), 400
        
#         # Rechercher
#         results = rag_searcher.get_detailed_results(query, top_k=top_k)
        
#         return jsonify({
#             'query': query,
#             'results': results,
#             'count': len(results)
#         })
        
#     except Exception as e:
#         return jsonify({
#             'error': str(e)
#         }), 500

# @app.route('/api/logs', methods=['GET'])
# def get_logs():
#     """
#     Route pour r√©cup√©rer les logs de conversation
#     Utile pour d√©boguer et analyser
#     """
#     try:
#         logs = []
#         log_file = 'logs/conversations.jsonl'
        
#         if os.path.exists(log_file):
#             with open(log_file, 'r', encoding='utf-8') as f:
#                 for line in f:
#                     if line.strip():
#                         logs.append(json.loads(line))
        
#         return jsonify({
#             'total': len(logs),
#             'logs': logs[-50:]  # Retourner les 50 derniers logs
#         })
        
#     except Exception as e:
#         return jsonify({
#             'error': str(e)
#         }), 500

# # ============== GESTION ERREURS ==============

# @app.errorhandler(404)
# def not_found(error):
#     """G√®re les routes non trouv√©es"""
#     return jsonify({
#         'error': 'Route non trouv√©e',
#         'path': request.path
#     }), 404

# @app.errorhandler(500)
# def server_error(error):
#     """G√®re les erreurs du serveur"""
#     return jsonify({
#         'error': 'Erreur du serveur',
#         'message': str(error)
#     }), 500

# # ============== MAIN ==============

# if __name__ == '__main__':
#     print("üöÄ D√©marrage du backend ADES ChatBot...")
#     print(f"üì° Connect√© √† Ollama: {OLLAMA_URL}")
#     print(f"ü§ñ Mod√®le: {OLLAMA_MODEL}")
#     print(f"üîç RAG: {'Activ√©' if RAG_ENABLED else 'D√©sactiv√©'}")
#     print("\nLe serveur d√©marre sur http://localhost:3000")
#     print("Appuyez sur Ctrl+C pour arr√™ter\n")
    
#     # Lancer le serveur
#     app.run(debug=True, host='0.0.0.0', port=3000)








# from flask import Flask, request, jsonify
# from flask_cors import CORS
# import requests
# import json
# import datetime
# import os
# from dotenv import load_dotenv
# from rag_search import RAGSearcher

# # Charger les variables d'environnement
# load_dotenv()

# # Cr√©er l'application Flask
# app = Flask(__name__)

# # Activer CORS (permet √† Flutter de communiquer)
# CORS(app)

# # ============== CONFIGURATION ==============

# # URL de Ollama (locale par d√©faut)
# OLLAMA_URL = os.getenv('OLLAMA_URL', 'http://localhost:11434')
# OLLAMA_MODEL = 'mistral'

# # System prompt : c'est les "instructions" qu'on donne √† Mistral
# SYSTEM_PROMPT = """Tu es l'Assistant Lemadio, un chatbot de support IA professionnel, concis et amical. Ton objectif principal est d'aider les vendeurs des points de vente ADES √† utiliser correctement l'application mobile "Lemadio".

# **R√®gles de R√©ponse :**

# 1.  **Synth√®se Professionnelle :** N'utilise JAMAIS les phrases exactes de la documentation fournie. Reformule les informations de mani√®re claire, √©tape par √©tape, et avec un ton serviable.
# 2.  **Ton et Style :** Utilise un langage professionnel, pr√©cis et simple (pas de jargon inutile). Adresse-toi directement √† l'utilisateur ("vous devez", "cliquez sur").
# 3.  **Clart√© des √âtapes :** Si la r√©ponse est une proc√©dure, utilise imp√©rativement des listes num√©rot√©es ou √† puces. Surligne (avec du gras) les mots-cl√©s (ex: **Valider**, **Nom d'utilisateur**, **Vente Directe**).
# 4.  **Gestion de l'Ambigu√Øt√© :** Si l'utilisateur pose une question trop g√©n√©rale (ex: "Comment cr√©er une vente ?"), suis la proc√©dure de clarification du document. Demande toujours : "Souhaitez-vous une **vente directe** ou une **vente revendeur** ?" avant de donner les √©tapes.
# 5.  **Pertinence du Contexte :** Ne r√©ponds qu'en te basant strictement sur les documents de r√©f√©rence fournis. Si une information n'est pas dans le document, r√©ponds poliment que tu ne disposes pas de cette information, sans deviner."""



# # Cr√©er le dossier logs s'il n'existe pas
# if not os.path.exists('logs'):
#     os.makedirs('logs')

# # ============== INITIALISATION RAG (VARIABLES GLOBALES) ==============

# print("\n" + "="*60)
# print("üöÄ D√âMARRAGE DU BACKEND ADES CHATBOT")
# print("="*60)

# # Initialiser le RAG Searcher
# try:
#     rag_searcher = RAGSearcher(data_dir='rag/data') 
#     RAG_ENABLED = True
#     print("‚úÖ RAG activ√© et pr√™t")
# except Exception as e:
#     print(f"‚ö†Ô∏è  RAG d√©sactiv√© : {e}")
#     print("   Le chatbot fonctionnera sans contexte personnalis√©")
#     rag_searcher = None
#     RAG_ENABLED = False # C'est l'initialisation de la variable globale

# print("="*60 + "\n")

# # ============== FONCTIONS UTILITAIRES ==============

# def log_conversation(user_message, bot_response, context_used=None):
#     """
#     Sauvegarde chaque conversation dans un fichier log
#     pour pouvoir analyser plus tard
#     """
#     timestamp = datetime.datetime.now().isoformat()
#     log_entry = {
#         'timestamp': timestamp,
#         'user': user_message,
#         'bot': bot_response,
#         'context_used': context_used is not None,
#         # RAG_ENABLED est lue ici, elle doit avoir √©t√© initialis√©e globalement
#         'rag_enabled': RAG_ENABLED 
#     }
    
#     log_file = 'logs/conversations.jsonl'
    
#     try:
#         with open(log_file, 'a', encoding='utf-8') as f:
#             f.write(json.dumps(log_entry, ensure_ascii=False) + '\n')
#     except Exception as e:
#         print(f"Erreur lors du logging: {e}")

# def call_ollama(prompt, temperature=0.3):
#     """
#     Appelle Ollama (Mistral) avec le prompt fourni
#     Retourne la r√©ponse
#     """
#     try:
#         # Pr√©parer la requ√™te pour Ollama
#         payload = {
#             'model': OLLAMA_MODEL,
#             'prompt': prompt,
#             'stream': False,
#             'temperature': temperature
#         }
        
#         # Appeler l'API Ollama
#         response = requests.post(
#             f'{OLLAMA_URL}/api/generate',
#             json=payload,
#             timeout=60  # Timeout apr√®s 60 secondes
#         )
        
#         # V√©rifier si l'appel a r√©ussi
#         if response.status_code == 200:
#             result = response.json()
#             return result.get('response', 'Erreur: pas de r√©ponse')
#         else:
#             return f'Erreur Ollama: {response.status_code}'
            
#     except requests.exceptions.ConnectionError:
#         return 'Erreur: Impossible de se connecter √† Ollama. V√©rifiez que Ollama fonctionne (ollama serve)'
#     except requests.exceptions.Timeout:
#         return 'Erreur: Ollama a mis trop de temps √† r√©pondre'
#     except Exception as e:
#         return f'Erreur: {str(e)}'

# def create_prompt_with_context(user_message, context):
#     """
#     Cr√©e le prompt complet avec le contexte RAG
#     """
#     prompt = f"""{SYSTEM_PROMPT}

# DOCUMENTATION ADES (extraits pertinents) :
# ---
# {context}
# ---

# QUESTION DU VENDEUR : {user_message}

# R√âPONSE (bas√©e uniquement sur la documentation ci-dessus) :"""
    
#     return prompt

# def create_prompt_without_context(user_message):
#     """
#     Cr√©e le prompt sans contexte (fallback si RAG d√©sactiv√©)
#     """
#     prompt = f"""{SYSTEM_PROMPT}

# QUESTION : {user_message}

# R√âPONSE :"""
    
#     return prompt

# # ============== ROUTES API ==============

# @app.route('/', methods=['GET'])
# def home():
#     """
#     Route simple pour v√©rifier que le serveur fonctionne
#     """
#     return jsonify({
#         'status': 'ok',
#         'message': 'Backend ADES ChatBot fonctionne!',
#         'version': '2.0.2',
#         'rag_enabled': RAG_ENABLED
#     })

# @app.route('/api/health', methods=['GET'])
# def health():
#     """
#     V√©rifie la sant√© du serveur et la connexion √† Ollama
#     """
#     try:
#         # Essayer de se connecter √† Ollama
#         response = requests.get(f'{OLLAMA_URL}/api/tags', timeout=5)
#         ollama_status = 'connected' if response.status_code == 200 else 'disconnected'
#     except:
#         ollama_status = 'disconnected'
    
#     # V√©rifier le statut du RAG
#     rag_status = 'disabled'
#     if RAG_ENABLED and rag_searcher:
#         try:
#             rag_health = rag_searcher.health_check()
#             rag_status = rag_health.get('status', 'error')
#         except:
#              rag_status = 'error_check'
    
#     return jsonify({
#         'backend': 'running',
#         'ollama': ollama_status,
#         'rag': rag_status,
#         'timestamp': datetime.datetime.now().isoformat()
#     })

# @app.route('/api/chat', methods=['POST'])
# def chat():
#     """
#     Route principale pour le chatbot avec RAG
#     Re√ßoit une question, cherche dans la doc, appelle Mistral, retourne la r√©ponse
#     """
#     # FIX: D√©clarer RAG_ENABLED comme globale pour pouvoir la modifier
#     global RAG_ENABLED 

#     try:
#         # R√©cup√©rer le message de la requ√™te
#         data = request.get_json()
#         user_message = data.get('message', '')
        
#         # Valider que le message n'est pas vide
#         if not user_message or len(user_message.strip()) == 0:
#             return jsonify({
#                 'error': 'Le message ne peut pas √™tre vide'
#             }), 400
        
#         # Variable pour stocker le contexte utilis√© (version texte)
#         context_used = None
        
#         # Cr√©er le prompt avec ou sans RAG
#         if RAG_ENABLED and rag_searcher:
            
#             print(f"üîç Recherche RAG pour : {user_message[:50]}...")
            
#             # Rechercher les chunks pertinents.
#             raw_chunks = rag_searcher.get_context(user_message, top_k=3, max_length=2000)
            
#             # V√©rification et extraction (conversion de la liste de dicts/strings en cha√Æne unique)
#             if isinstance(raw_chunks, list) and all(isinstance(item, dict) and 'text' in item for item in raw_chunks):
#                 # Si c'est une liste de dictionnaires (cas o√π RAGSearcher.get_context est modifi√© pour retourner des dicts)
#                 context_used = "\n---\n".join([chunk['text'] for chunk in raw_chunks])
#                 print(f"‚úÖ Contexte trouv√© (liste de dicts) ({len(context_used)} caract√®res)")
#             elif isinstance(raw_chunks, str):
#                 # Si c'est d√©j√† une cha√Æne (cas par d√©faut de RAGSearcher.get_context)
#                 context_used = raw_chunks
#                 print(f"‚úÖ Contexte trouv√© (cha√Æne) ({len(context_used)} caract√®res)")
#             else:
#                 # Fallback si le format est inattendu
#                 print(f"‚ùå Erreur de format de contexte RAG. Utilisation du prompt basique.")
#                 # Si le format est incorrect, on d√©sactive RAG pour cette requ√™te et la prochaine
#                 RAG_ENABLED = False 

#             if context_used:
#                 full_prompt = create_prompt_with_context(user_message, context_used)
#             else:
#                  full_prompt = create_prompt_without_context(user_message)
                 
#         else:
#             # Fallback sans RAG
#             full_prompt = create_prompt_without_context(user_message)
#             print(f"‚ö†Ô∏è  Pas de RAG, utilisation du prompt basique")
        
#         # Appeler Ollama
#         print(f"ü§ñ Appel √† Mistral...")
#         bot_response = call_ollama(full_prompt)
#         print(f"‚úÖ R√©ponse re√ßue")
        
#         # Logger la conversation
#         log_conversation(user_message, bot_response, context_used)
        
#         # Pr√©parer la r√©ponse
#         response_data = {
#             'status': 'success',
#             'user_message': user_message,
#             'reply': bot_response,
#             # RAG_ENABLED est accessible ici car elle est d√©clar√©e globale.
#             'rag_used': RAG_ENABLED and context_used is not None, 
#             'timestamp': datetime.datetime.now().isoformat()
#         }
        
#         # Ajouter le contexte si demand√© (pour d√©boguer)
#         if data.get('include_context', False) and context_used:
#             response_data['context'] = context_used
        
#         return jsonify(response_data)
        
#     except Exception as e:
#         print(f"‚ùå Erreur dans /api/chat : {e}")
#         return jsonify({
#             'status': 'error',
#             'message': str(e)
#         }), 500

# @app.route('/api/search', methods=['POST'])
# def search():
#     """
#     Route pour tester la recherche RAG directement
#     Utile pour d√©boguer
#     """
#     try:
#         if not RAG_ENABLED or not rag_searcher:
#             return jsonify({
#                 'error': 'RAG non disponible'
#             }), 503
        
#         data = request.get_json()
#         query = data.get('query', '')
#         top_k = data.get('top_k', 3)
        
#         if not query:
#             return jsonify({
#                 'error': 'Query manquante'
#             }), 400
        
#         # Rechercher
#         results = rag_searcher.get_detailed_results(query, top_k=top_k)
        
#         return jsonify({
#             'query': query,
#             'results': results,
#             'count': len(results)
#         })
        
#     except Exception as e:
#         return jsonify({
#             'error': str(e)
#         }), 500

# @app.route('/api/logs', methods=['GET'])
# def get_logs():
#     """
#     Route pour r√©cup√©rer les logs de conversation
#     Utile pour d√©boguer et analyser
#     """
#     try:
#         logs = []
#         log_file = 'logs/conversations.jsonl'
        
#         if os.path.exists(log_file):
#             with open(log_file, 'r', encoding='utf-8') as f:
#                 for line in f:
#                     if line.strip():
#                         logs.append(json.loads(line))
        
#         return jsonify({
#             'total': len(logs),
#             'logs': logs[-50:]  # Retourner les 50 derniers logs
#         })
        
#     except Exception as e:
#         return jsonify({
#             'error': str(e)
#         }), 500

# # ============== GESTION ERREURS ==============

# @app.errorhandler(404)
# def not_found(error):
#     """G√®re les routes non trouv√©es"""
#     return jsonify({
#         'error': 'Route non trouv√©e',
#         'path': request.path
#     }), 404

# @app.errorhandler(500)
# def server_error(error):
#     """G√®re les erreurs du serveur"""
#     return jsonify({
#         'error': 'Erreur du serveur',
#         'message': str(error)
#     }), 500

# # ============== MAIN ==============

# if __name__ == '__main__':
#     print("üöÄ D√©marrage du backend ADES ChatBot...")
#     print(f"üì° Connect√© √† Ollama: {OLLAMA_URL}")
#     print(f"ü§ñ Mod√®le: {OLLAMA_MODEL}")
#     print(f"üîç RAG: {'Activ√©' if RAG_ENABLED else 'D√©sactiv√©'}")
#     print("\nLe serveur d√©marre sur http://localhost:3000")
#     print("Appuyez sur Ctrl+C pour arr√™ter\n")
    
#     # Lancer le serveur
#     app.run(debug=True, host='0.0.0.0', port=3000)






from flask import Flask, request, jsonify
from flask_cors import CORS
import requests
import json
import datetime
import os
from dotenv import load_dotenv
from rag_search import RAGSearcher

# Charger les variables d'environnement
load_dotenv()

# Cr√©er l'application Flask
app = Flask(__name__)
CORS(app)

# ============== CONFIGURATION ==============

# Mode de d√©ploiement : utilise HuggingFace au lieu d'Ollama
USE_HUGGINGFACE = os.getenv('USE_HUGGINGFACE', 'false').lower() == 'true'
HUGGINGFACE_API_KEY = os.getenv('HUGGINGFACE_API_KEY', '')
HUGGINGFACE_MODEL = "mistralai/Mistral-7B-Instruct-v0.2"

# Ollama (pour d√©veloppement local)
OLLAMA_URL = os.getenv('OLLAMA_URL', 'http://localhost:11434')
OLLAMA_MODEL = 'mistral'

# System prompt
SYSTEM_PROMPT = """Tu es un assistant d'aide pour l'application mobile Lemadio d'ADES.
Tu aides les vendeurs √† utiliser l'application de vente.

R√àGLES IMPORTANTES :
- Base tes r√©ponses UNIQUEMENT sur la documentation fournie ci-dessous
- Sois pr√©cis, clair et concis
- R√©ponds en fran√ßais ou malagasy selon la question
- Si tu ne trouves pas l'information dans la documentation, dis-le honn√™tement
- Donne des instructions √©tape par √©tape quand c'est appropri√©"""

# Cr√©er le dossier logs s'il n'existe pas
if not os.path.exists('logs'):
    os.makedirs('logs')

# ============== INITIALISATION RAG ==============

print("\n" + "="*60)
print("üöÄ D√âMARRAGE DU BACKEND ADES CHATBOT")
print("="*60)
print(f"Mode: {'Production (HuggingFace)' if USE_HUGGINGFACE else 'D√©veloppement (Ollama)'}")

# Initialiser le RAG Searcher
try:
    rag_searcher = RAGSearcher(data_dir='rag/data')
    RAG_ENABLED = True
    print("‚úÖ RAG activ√© et pr√™t")
except Exception as e:
    print(f"‚ö†Ô∏è  RAG d√©sactiv√© : {e}")
    rag_searcher = None
    RAG_ENABLED = False

print("="*60 + "\n")

# ============== FONCTIONS UTILITAIRES ==============

def log_conversation(user_message, bot_response, context_used=None):
    """Sauvegarde chaque conversation"""
    timestamp = datetime.datetime.now().isoformat()
    log_entry = {
        'timestamp': timestamp,
        'user': user_message,
        'bot': bot_response,
        'context_used': context_used is not None,
        'rag_enabled': RAG_ENABLED
    }
    
    log_file = 'logs/conversations.jsonl'
    
    try:
        with open(log_file, 'a', encoding='utf-8') as f:
            f.write(json.dumps(log_entry, ensure_ascii=False) + '\n')
    except Exception as e:
        print(f"Erreur lors du logging: {e}")

def call_huggingface(prompt):
    """Appelle HuggingFace Inference API"""
    try:
        headers = {"Authorization": f"Bearer {HUGGINGFACE_API_KEY}"}
        
        payload = {
            "inputs": prompt,
            "parameters": {
                "max_new_tokens": 512,
                "temperature": 0.3,
                "top_p": 0.95,
                "return_full_text": False
            }
        }
        
        response = requests.post(
            f"https://api-inference.huggingface.co/models/{HUGGINGFACE_MODEL}",
            headers=headers,
            json=payload,
            timeout=30
        )
        
        if response.status_code == 200:
            result = response.json()
            if isinstance(result, list) and len(result) > 0:
                return result[0].get('generated_text', 'Erreur: pas de r√©ponse')
            return 'Erreur: format de r√©ponse invalide'
        else:
            return f'Erreur HuggingFace: {response.status_code}'
            
    except Exception as e:
        return f'Erreur: {str(e)}'

def call_ollama(prompt):
    """Appelle Ollama (d√©veloppement local)"""
    try:
        payload = {
            'model': OLLAMA_MODEL,
            'prompt': prompt,
            'stream': False,
            'temperature': 0.3
        }
        
        response = requests.post(
            f'{OLLAMA_URL}/api/generate',
            json=payload,
            timeout=30
        )
        
        if response.status_code == 200:
            result = response.json()
            return result.get('response', 'Erreur: pas de r√©ponse')
        else:
            return f'Erreur Ollama: {response.status_code}'
            
    except Exception as e:
        return f'Erreur: {str(e)}'

def call_llm(prompt):
    """Appelle le LLM appropri√© selon l'environnement"""
    if USE_HUGGINGFACE:
        return call_huggingface(prompt)
    else:
        return call_ollama(prompt)

def create_prompt_with_context(user_message, context):
    """Cr√©e le prompt complet avec le contexte RAG"""
    prompt = f"""{SYSTEM_PROMPT}

DOCUMENTATION LEMADIO (extraits pertinents) :
---
{context}
---

QUESTION DU VENDEUR : {user_message}

R√âPONSE (bas√©e uniquement sur la documentation ci-dessus) :"""
    
    return prompt

# ============== ROUTES API ==============

@app.route('/', methods=['GET'])
def home():
    """Route simple pour v√©rifier que le serveur fonctionne"""
    return jsonify({
        'status': 'ok',
        'message': 'Backend ADES ChatBot Lemadio fonctionne!',
        'version': '2.0.0',
        'rag_enabled': RAG_ENABLED,
        'mode': 'production' if USE_HUGGINGFACE else 'development'
    })

@app.route('/api/health', methods=['GET'])
def health():
    """V√©rifie la sant√© du serveur"""
    llm_status = 'connected'
    
    if not USE_HUGGINGFACE:
        try:
            response = requests.get(f'{OLLAMA_URL}/api/tags', timeout=5)
            llm_status = 'connected' if response.status_code == 200 else 'disconnected'
        except:
            llm_status = 'disconnected'
    
    rag_status = 'disabled'
    if RAG_ENABLED and rag_searcher:
        rag_health = rag_searcher.health_check()
        rag_status = rag_health.get('status', 'error')
    
    return jsonify({
        'backend': 'running',
        'llm': llm_status,
        'rag': rag_status,
        'mode': 'production' if USE_HUGGINGFACE else 'development',
        'timestamp': datetime.datetime.now().isoformat()
    })

@app.route('/api/chat', methods=['POST'])
def chat():
    """Route principale pour le chatbot avec RAG"""
    try:
        data = request.get_json()
        user_message = data.get('message', '')
        
        if not user_message or len(user_message.strip()) == 0:
            return jsonify({
                'error': 'Le message ne peut pas √™tre vide'
            }), 400
        
        context_used = None
        
        # Cr√©er le prompt avec ou sans RAG
        if RAG_ENABLED and rag_searcher:
            print(f"üîç Recherche RAG pour : {user_message[:50]}...")
            context = rag_searcher.get_context(user_message, top_k=3, max_length=2000)
            context_used = context
            
            full_prompt = create_prompt_with_context(user_message, context)
            print(f"‚úÖ Contexte trouv√© ({len(context)} caract√®res)")
        else:
            full_prompt = f"{SYSTEM_PROMPT}\n\nQUESTION: {user_message}\n\nR√âPONSE:"
            print(f"‚ö†Ô∏è  Pas de RAG")
        
        # Appeler le LLM
        print(f"ü§ñ Appel au LLM...")
        bot_response = call_llm(full_prompt)
        print(f"‚úÖ R√©ponse re√ßue")
        
        # Logger la conversation
        log_conversation(user_message, bot_response, context_used)
        
        return jsonify({
            'status': 'success',
            'user_message': user_message,
            'reply': bot_response,
            'rag_used': RAG_ENABLED and context_used is not None,
            'timestamp': datetime.datetime.now().isoformat()
        })
        
    except Exception as e:
        print(f"‚ùå Erreur dans /api/chat : {e}")
        return jsonify({
            'status': 'error',
            'message': str(e)
        }), 500

@app.route('/api/search', methods=['POST'])
def search():
    """Route pour tester la recherche RAG directement"""
    try:
        if not RAG_ENABLED or not rag_searcher:
            return jsonify({
                'error': 'RAG non disponible'
            }), 503
        
        data = request.get_json()
        query = data.get('query', '')
        top_k = data.get('top_k', 3)
        
        if not query:
            return jsonify({
                'error': 'Query manquante'
            }), 400
        
        results = rag_searcher.get_detailed_results(query, top_k=top_k)
        
        return jsonify({
            'query': query,
            'results': results,
            'count': len(results)
        })
        
    except Exception as e:
        return jsonify({
            'error': str(e)
        }), 500

@app.route('/api/logs', methods=['GET'])
def get_logs():
    """Route pour r√©cup√©rer les logs de conversation"""
    try:
        logs = []
        log_file = 'logs/conversations.jsonl'
        
        if os.path.exists(log_file):
            with open(log_file, 'r', encoding='utf-8') as f:
                for line in f:
                    if line.strip():
                        logs.append(json.loads(line))
        
        return jsonify({
            'total': len(logs),
            'logs': logs[-50:]
        })
        
    except Exception as e:
        return jsonify({
            'error': str(e)
        }), 500

# ============== GESTION ERREURS ==============

@app.errorhandler(404)
def not_found(error):
    return jsonify({
        'error': 'Route non trouv√©e',
        'path': request.path
    }), 404

@app.errorhandler(500)
def server_error(error):
    return jsonify({
        'error': 'Erreur du serveur',
        'message': str(error)
    }), 500

# ============== MAIN ==============

if __name__ == '__main__':
    port = int(os.environ.get('PORT', 3000))
    print(f"üöÄ Serveur d√©marrant sur le port {port}")
    app.run(debug=False, host='0.0.0.0', port=port)